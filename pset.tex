\documentclass{article}[10pt]

\usepackage{amsthm}
\theoremstyle{definition}\newtheorem{question}{Question}
\renewcommand*{\proofname}{Solution}

\usepackage[margin=1in,footskip=0.25in]{geometry}

\usepackage{hyperref}

\renewcommand{\ttdefault}{cmtt}

\begin{document}

\begin{question}
X-drop alignment attempts to find the best matching prefixes between two strings.
During the alignment, we keep track of the best alignment score $S$ and stop the alignment
if the score of the current cell drops below $S-X$, where $X$ is the drop cutoff.
If the best matching prefixes are much shorter than the two input strings,
we only need to compute a small number of cells without filling the entire scoring matrix.

Suppose the match score is 1 and the mismatch/gap penalty is 2.
Under X-drop cutoff 10, perform X-drop alignment between the following strings and
output the optimal alignment score and the lengths of the matching prefixes.
\begin{quote}
\footnotesize\tt
GTTGATGGTCTACAACGTTATCGTCACAGCCCATGCATTTGTAATAATCTTCTTCATAGTAATA\\
GATAGATGGTCTGAGCTATGATATCAATTGGCTTCCTAGGGTTTATCGTGTGAGCACACCATATT
\end{quote}
Requirements:
\begin{itemize}
\item Find the correct solution ({\bf 5pt})
\item Avoid filling the entire scoring matrix ({\bf 3pt})
\item As detailed base alignment is not needed, reduce memory by using 1-dimensional array ({\bf 3pt})
\end{itemize}
\end{question}

\begin{proof}
See \url{https://ideone.com/0h4m0f} (in Javascript).
The alignment score is 6 and the best matching prefixes are:
\begin{quote}
\footnotesize\tt
GTTGATGGTCT   (of length 11)\\
GATAGATGGTCT  (of length 12)
\end{quote}
\end{proof}



%\begin{question}
%Given a string $T$, a \emph{subsequence} of $T$ is composed of symbols in $T$
%without changing their original order in $T$. For example, both ${\tt CGA}$ and
%${\tt CCT}$ are subsequences of ${\tt CGCTA}$. Given two strings $T_1$ and
%$T_2$, a \emph{common subsequence} is a string that is the subsequence of both
%$T_1$ and $T_2$. For example, ${\tt CGA}$ is a common subsequence of ${\tt
%CGCTA}$ and ${\tt CTGAT}$. The 2-string longest common subsequence problem
%is to find the longest common subsequence (LCS) among all subsequences between
%two strings. Questions:
%\begin{enumerate}
%\item Find the length of LCS between two strings with dynamic programming (DP).
%Write down the DP equation. \emph{Hint: very similar to computing edit distance.}
%\item Implement the algorithm and compute the length of LCS between 
%``${\tt GCATAAATTATAACTCCAGCTATAGC}$'' and ``${\tt CAAACCCCAGGGACGGGTTAGCA}$''.
%Try to use less memory.
%\end{enumerate}
%\end{question}
%
%\begin{proof}
%Given two strings $A=a_1a_2\ldots a_m$ and $B=b_1b_2\ldots b_n$, let $f(i,j)$
%be the length of LCS between prefixes $a_1a_2\ldots a_i$ and $b_1b_2\ldots
%b_j$. Then $f(m,n)$ gives the length of LCS between $A$ and $B$.
%
%The computation of $f()$ starts with $f(i,0)=0$ and $f(0,j)=0$ for $0\le i\le m$
%and $0\le j\le n$. The DP equation is
%$$
%f(i,j)=\max\{f(i-1,j-1)+\delta(a_i,b_j),f(i-1,j),f(i,j-1)\}
%$$
%where $\delta(a,b)$ equals 1 if $a=b$, or 0 otherwise.
%
%A python implementation can be found at
%\href{https://ideone.com/Om4JhD}{https://ideone.com/Om4JhD}. For the two given
%strings, the length of LCS is 15.
%
%An equivalent equation is
%$$
%f(i,j)=\left\{\begin{array}{ll}
%0            & \mbox{if $i=0$ or $j=0$}\\
%f(i-1,j-1)+1 & \mbox{if $i,j>0$ and $a_i=b_j$}\\ 
%\max\{f(i-1,j),f(i,j-1)\} & \mbox{otherwise}
%\end{array}\right.
%$$
%It is equivalent because $f(i-1,j-1)\le\max\{f(i-1,j),f(i,j-1)\}$.
%\end{proof}



\begin{question}
Suppose there are $n$ distinct $k$-mers in two DNA sequences and $m$ of which are
shared between the two sequences. Questions:
\begin{enumerate}
\item Prove that the similarity between the two sequences can be estimated by
the following equation, assuming no $k$-mers occurring multiple times on one
sequence.
$$
\hat{p}=\left(\frac{2m}{n+m}\right)^{1/k}
$$
\item Implement the algorithm and find the sequence similarity between the following two strings for $k=7$:
\begin{quote}
\footnotesize\tt
CGATATAAGCAATATTATTATATTACGCCCAATAAACGATAGCTAATATGCCGCGGAGCGATCGAACCGCCCCGATAAGACCCC\\
CGATATAAGCAATATTAaTATATTAGCCCAATAAACGATAGCTAATATGggGCGGAGCGATCGAACCGCCCCGAaaTAAGACCCC
\end{quote}
\item Discuss potential problems with this approximation.
\end{enumerate}
\end{question}

\begin{proof}
If there are no $k$-mers occurring multiple times in one sequence, there are
$n+m$ $k$-mers in total in the two sequences and $2m$ of them are shared.
Let the similarity be $p$. The probability of seeing no differences on a
$k$-long sequence is $p^k$. The likelihood function is
$$
\mathcal{L}(p)=\left(p^k\right)^{2m}\left(1-p^k\right)^{n+m-2m}=p^{2km}\left(1-p^k\right)^{n-m}
$$
Therefore,
$$
\frac{\partial\log\mathcal{L}(p)}{\partial p}=\frac{2km}{p}-\frac{n-m}{1-p^k}\cdot kp^{k-1}=\frac{k\cdot\left[2m-(n+m)p^k\right]}{p\left(1-p^k\right)}
$$
The maximum likelihood estimate of $p$ is obtained at
$\partial\log\mathcal{L}/\partial p=0$. Solving the equation gives
$$
\hat{p}=\left(\frac{2m}{n+m}\right)^{1/k}
$$

An alternative derivation can use a Poisson approximation. In this case, the
probability of seeing no differences on a $k$-mer is $e^{-pk}$. The maximum
likelihood estimate is $-\frac{1}{k}\log\frac{2m}{m+n}$. We can find this
approximation in the Mash paper (Ondov et al, 2016). The first derivation is
preferred as it uses one less approximation.

A sorting-based python implementation is available at
\url{https://ideone.com/F02WCa}. The 7-mer
similarity is 93.8\%. This algorithm can also implemented with a hash table.
The major problem with this approximation is that it assumes all $k$-mers are
independent. Due to actual $k$-mer dependencies, the estimate varies with $k$.
\end{proof}



\begin{question}
Suppose there are three transcripts of 200bp, 500bp and 300bp in length, respectively.
Five reads are mapped to these transcripts. Read A and B are mapped to transcript 1 and 2,
read C and D to transcript 2 only and read E is mapped to transcript 3 only.
Use EM to estimate the transcript fraction of the three transcripts.
\end{question}

\begin{proof}
25\%, 50\% and 25\%. Source code: \url{https://ideone.com/EKB95Q}.
\end{proof}



\begin{question}
Implement a simplified CNV caller using HMM. Suppose after read mapping, we
symbolize a series of loci into a sequence like ``${\tt
11111110102222211110111}$'' where symbol `${\tt 1}$' for normal coverage,
`${\tt 0}$' for lower than expected coverage and `${\tt 2}$' for higher
coverage. We design a 2-state HMM to call duplications on the symbol sequence.
The HMM has two hidden states: `${\tt N}$' for normal ploidy and `${\tt D}$' for
duplications. We parameterize the HMM as follows, where $P$ gives the transition
probabilities between hidden states and $E$ gives the emission probabilities:
$$
{\bf P}=\left(\begin{array}{ll}
p({\tt N}|{\tt N}) & p({\tt D}|{\tt N}) \\
p({\tt N}|{\tt D}) & p({\tt D}|{\tt D})
\end{array}\right)
=\left(\begin{array}{ll}
0.9 & 0.1 \\
0.2 & 0.8
\end{array}\right)
$$
\end{question}
$$
{\bf E}=\left(\begin{array}{lll}
e({\tt 0}|{\tt N}) & e({\tt 1}|{\tt N}) & e({\tt 2}|{\tt N})\\
e({\tt 0}|{\tt D}) & e({\tt 1}|{\tt D}) & e({\tt 2}|{\tt D})
\end{array}\right)
=\left(\begin{array}{lll}
0.15 & 0.70 & 0.15\\
0.01 & 0.19 & 0.80
\end{array}\right)
$$
Questions:
\begin{enumerate}
\item Derive the probability of the hidden state if we run the HMM for infinite
time (i.e. the stationary distribution).
\item Write a program to infer the hidden states of symbol sequence with posterior decoding. 
\begin{quote}
\footnotesize\tt 1211100122111011112222112022010200111001211202220112112111220222112221211121002122122222222212222112\\
\end{quote}

Here is a sample output for a different input
\begin{quote}
\footnotesize\tt 1012111121101211001111121120012222222222212221220111211111111002212222221112212122111111120111222122
\footnotesize\tt NNNNNNNNNNNNNNDDNNNNNNNNNNNNNNDDDDDDDDDDDDDDDDDDNNNNNNNNNNNNNNNDDDDDDDDNNNDDDDDNDDNNNNNNNNNNNNDDDDDD
\end{quote}
\end{enumerate}

\begin{proof}
The stationary distribution ${\bf q}=(q({\tt N}),q({\tt D}))$ follows
$$
{\bf q}\cdot{\bf P}={\bf q}
$$
on the condition that $q({\tt N})+q({\tt D})=1$. Solving this equation gives
$$
q({\tt N})=\frac{p({\tt N}|{\tt D})}{p({\tt N}|{\tt D}) + p({\tt D}|{\tt N})}
$$
In this example, $q({\tt N})=2/3$.

For the given string, the inferred hidden states are:
\begin{quote}
\footnotesize\tt 1211100122111011112222112022010200111001211202220112112111220222112221211121002122122222222212222112\\
\footnotesize\tt NNNNNNNNDDNNNNNNNNDDDDNNNNDNNNNNNNNNNNNNNNNNNDDDNNNNNNNNNNDDNDDDDDDDDDDNNNNNNNNNDDDDDDDDDDDDDDDDDNNN
\end{quote}
A python implementation is available at \url{https://ideone.com/nnh6Q2}.

\begin{flushleft}
PS: This is simulated data. The truth actually is
\end{flushleft}
\begin{quote}
\footnotesize\tt 1211100122111011112222112022010200111001211202220112112111220222112221211121002122122222222212222112\\
\footnotesize\tt NNNNNNNNNDDNNNNNNNNDDDNNNNNNNNNNNNNNNNNNNNNNNDDDNNNNNNNNNDDDNDDDDDDDDDDNNNNNNNDDDDDDDDDDDDDDDDDDDDDD
\end{quote}

The simulator and the posterior decoder can be found at
\url{https://ideone.com/a9a2h0}. Students do not
need to provide a simulator.

\end{proof}


%\begin{question}
%Suppose we have five reads over seven SNPs:
%\begin{quote}
%\tt RRAAAR.\\
%\tt AA.RRAR\\
%\tt ARARARA\\
%\tt .ARRAAR\\
%\tt .ARRRAA
%\end{quote}
%where each line represents a read, ``{\tt R}'' stands for a reference allele,
%``{\tt A}'' for an alternate allele, and ``{\tt .}'' for a missing allele. Use
%the iterative flipping algorithm to find the phase of each SNP and the number
%of minimum errors. Are there other local minima?
%\end{question}
%
%\begin{proof}
%The best solution is (or its complement):
%\begin{quote}
%\tt X ++---+-\\
%\tt + RRAAAR.\\
%\tt - AA.RRAR\\
%\tt + \underline{A}RA\underline{R}ARA\\
%\tt - .ARR\underline{A}AR\\
%\tt - .ARRRA\underline{A}
%\end{quote}
%The four underlined letters indicate the errors. There are other local minima
%with 11 errors. Source code in JavaScript can be found at
%\href{https://ideone.com/2wqlig}{https://ideone.com/2wqlig}.
%\end{proof}


\begin{question}
Given the multiple-sequence alignment (MSA) of D-loop segments in primate mitochondria
\begin{quote}
\footnotesize\verb|human   TCCTGCCTCATCCTATTATTTATCGCACCTAC-GTTCAATATTACAGGCGAA-CATA-CTTACTAAAGTGTGTTAATTAATTAATGCTTGTAG|\\
\footnotesize\verb|bonobo  TCCTGCCCCATTACGTTATTTATCGCACCTAC-GTTCAATATTATTACCTAG-CATGATTTACTAAAGCGTGTTAATTAATTAATGCTTGTAG|\\
\footnotesize\verb|chimp   TCCTGCCCCATTGTATTATTTATCGCACCTAC-GTTCAATATTACGACCTAG-CATA-CCTACTAAAGTGTGTTGATTAATTAATGCTTGCAG|\\
\footnotesize\verb|gibbon  TTCTGACCCATCCTATTGTTGATCGCGCCTAC-GTTCAATATCCCAGCCGAG-CATA-CTTACACTAAGGTGTTAATTAATTCATGCTTGTTG|\\
\footnotesize\verb|oran-pa TCCTACCTCATGCCATTATTAATCGCGCCTAATATCCAATATCCTAGCCCCACCCTC-AGTGTTTGAAGCTGCTATTTAATTTATGCTAG-AG|\\
\footnotesize\verb|gorilla TCCTGCCCCATGCTACCATTTATCGCACCTAC-GTTCAATATTACAGCCGAG-CGCA-CAGTGTTCATGGTGTTAATTAATTCATGCTTGTTG|\\
\footnotesize\verb|oran-pp TCCTGCCCCATGGCGTTATTGATCGCGCCTAACGTCCAATGTTCTAGCGCCC-CCTC-CCTATTGAAAGTTGTTATTTAATTTATGCTAG-AG|
\end{quote}
and candidate tree topologies
\begin{quote}
\footnotesize\tt ((((human,(chimp,bonobo)),gorilla),(oran-pa,oran-pp)),gibbon)\\
\footnotesize\tt ((((human,gorilla),(chimp,bonobo)),(oran-pa,oran-pp)),gibbon)\\
\footnotesize\tt ((((human,bonobo),gorilla),(oran-pa,oran-pp)),(chimp,gibbon))\\
\footnotesize\tt (human,(((gibbon,(oran-pa,oran-pp)),gorilla),(chimp,bonobo)))
\end{quote}
Write a program to compute the cost of each candidate tree using Fitch's maximum-parsimony (MP) algorithm.
Which topoloy is the optimal under MP? Can we use MP to root an unrooted phylogenetic tree?
Additional requirement: parse tree strings in the New Hampshire format; don't hard-code the trees in the source code.
\end{question}

\begin{proof}
The costs are 84, 87, 94 and 84, respectively. The first and the last topologies are both optimal.
They in fact have identical unrooted topology, so MP can't be used to root a tree.
Source code at \url{https://ideone.com/TnhZ6O} (in JavaScript).
ChatGPT generated a hard-coded version, \url{https://ideone.com/0xlpTl}, which is quite interesting.
\end{proof}


\end{document}
