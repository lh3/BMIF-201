\documentclass[10pt]{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{comment}

\usepackage{hyperref}

\renewcommand{\ttdefault}{cmtt}

\begin{document}

\section{Probabilistic}

\subsection{Conditional probability and Bayesian formula}

Joint probability, written as $P(A\cap B)$ or $P(A,B)$, means two events happening at the
same time.  Conditional probability is defined as:
$$
P(A|B)\triangleq\frac{P(A,B)}{P(B)}
$$
Then
$$
P(A,B)=P(A|B)P(B)=P(B|A)P(A)
$$
thus Bayesian formula:
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

\begin{comment}
An example. Let
$$
X=\left\{\begin{array}{ll}
1 & \mbox{if sunny} \\
0 & \mbox{otherwise}
\end{array}\right.
$$
$$
Y=\left\{\begin{array}{ll}
1 & \mbox{if warm} \\
0 & \mbox{if cold}
\end{array}\right.
$$
$$
\Pr\{\mbox{sunny}\}=\Pr\{\mbox{sunny and warm}\}+\Pr\{\mbox{sunny and cold}\}
$$
$$
\Pr\{X=x\}=\sum_y\Pr\{X=x,Y=y\}
$$
\end{comment}

\subsection{Likelihood and maximum likelihood estimation (MLE)}

A likelihood function or simply \emph{likelihood} is the probability of data
give parameters. It is a function of parameters.
$$
\mathcal{L}(\theta)=\Pr\{d|\theta\}
$$
MLE finds $\hat{\theta}$ that maximizes $\mathcal{L}(\theta)$.  The estimate
$\hat{\theta}$ satisfies:
$$
\left.\frac{\partial \mathcal{L}(\theta)}{\partial\theta}\right|_{\theta=\hat{\theta}}=0
$$
Or sometimes for the ease of derivaton:
$$
\left.\frac{\partial \log\mathcal{L}(\theta)}{\partial\theta}\right|_{\theta=\hat{\theta}}=0
$$

\begin{comment}
An example. Suppose in the last August, 20 out of 31 days were sunny.
\end{comment}

\subsection{The estimation-maximization algorithm (EM)}

For a complex problem, it is often difficult to write down the close form of
$\mathcal{L}(\theta)$, so we can't directly do derivation. EM is an iterative
algorithm to find MLE when the likihood function can be expressed with
latent/hidden variables.
$$
\mathcal{L}(\theta)=\Pr\{{\bf d}|\theta\}=\sum_z\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}
$$
where $\sum_{\bf z}$ sums over all latent states. For contiguous latent
variables, replace sum with integral.

The central equation in EM is the $Q$ funciton defined by
$$
Q(\theta|\theta^t)\triangleq\mathbb{E}_{{\bf z}|{\bf d},\theta^t}\log\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}=\sum_{{\bf z}}\Pr\{{\bf Z}={\bf z}|{\bf d},\theta^t\}\cdot\log\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}
$$
As we will show in the next section, if we choose $\theta=\theta^{t+1}$ such that
$$
\left.\frac{\partial Q(\theta|\theta^t)}{\partial\theta}\right|_{\theta=\theta^{t+1}}=0
$$
we have $\mathcal{L}(\theta^{t+1})\ge\mathcal{L}(\theta^t)$.

For a problem solvable by EM, $Q(\theta|\theta^t)$ can usually be expressed in
the close form or take a simpler form than $\mathcal{L}(\theta)$. EM guarantees
convergence, but doesn't guarantee to converge to the global maxmium.

\subsection{Proving EM}

Here is the derivation of the EM algorithm in Durbin et al (1998). Because
$P(d,z|\theta)=P(z|d,\theta)P(d|\theta)$, we have
$$
\log P(d|\theta)=\log P(d,z|\theta)-\log P(z|d,\theta)
$$
Multiplying $P(z|d,\theta^t)$ and summing over $z$
$$
\log P(d|\theta)=\sum_zP(z|d,\theta^t)\log P(d,z|\theta) - \sum_zP(z|d,\theta^t)\log P(z|d,\theta)
$$
The first term on the right is the $Q$ function. Then
$$
\log P(d|\theta)-\log P(d|\theta^t)=Q(\theta|\theta^t)-Q(\theta^t|\theta^t)+\sum_z P(z|d,\theta^t)\log\frac{P(z|x,\theta^t)}{P(z|d,\theta)}
$$
The third term on the right is the relative entropy (aka Kullback-Leibler
distance), which is always non-negative. Therefore,
$$
\log P(d|\theta)-\log P(d|\theta^t)\ge Q(\theta|\theta^t)-Q(\theta^t|\theta^t)
$$
If we choose $\theta=\theta^{t+1}$ such that
$Q(\theta^{t+1}|\theta^t)>Q(\theta^t|\theta^t)$,
$P(d|\theta^{t+1})>P(d|\theta^t)$. i.e. $\theta^{t+1}$ guarantees increased
likelihood. For a standard EM, we typically choose $\theta^{t+1}$ at the $(t+1)$
iteration such that it maximizes $Q(\theta|\theta^t)$.

\section{Resolving ambiguously mapped reads for RNA-seq}

Suppose we have $N$ transcripts and $R$ reads.
Define
$$
\delta_{it}=\left\{\begin{array}{ll}
1 & \mbox{if read $i$ hits transcript $t$} \\
0 & \mbox{otherwise}
\end{array}\right.
$$
Let $p_t$ be the probability of transcript $t$, $t=1,\ldots,N$.
The probability of a read being sequenced from $t$ is
$$
q_t=\frac{p_tl_t}{\sum_s{p_sl_s}}
$$
Suppose we know $\{p_t\}$, the expected value $\{\bar{q}_t\}$ can be calculated with
$$
\bar{q}_t=\frac{1}{R}\sum_i\frac{p_t\delta_{it}}{\sum_s{p_s\delta_{is}}}
=\frac{1}{R}\sum_i\frac{(q_t/l_t)\cdot\delta_{it}}{\sum_s{(q_s/l_s)\cdot\delta_{is}}}
$$
Once we get the estimate of $\{q_t\}$, we can calculate $\{p_t\}$ with
$$
p_t=\frac{q_t/l_l}{\sum_s q_s/l_s}
$$
The TPM of transcript $t$ equals $p_t\cdot10^6$.

\newpage

\section{Markov Chain}

\subsection{Markov chain: the basic}
Let $\Sigma$ be an alphabet and $a\in\Sigma$ be a symbol. Markov property:
$$
\Pr\{X_i=a_i|X_{1,i-1}=a_1\cdots a_{i-1}\}=\Pr\{X_i=a_i|X_{i-1}=a_{i-1}\}
$$
A Markov chain is \emph{homogeneous} if
$$
\Pr\{X_i=a_i|X_{i-1}=a_{i-1}\}=\Pr\{X_j=a_j|X_{j-1}=a_{j-1}\}
$$
for any $i$ and $j$. Let $p_{ab}=p(b|a)\triangleq\Pr\{X_i=b|X_{i-1}=a\}$ be the
transition probability from $a$ to $b$. Then
$$
P(x)=\Pr\{X_{1,L}=a_1\cdots a_L\}=q(a_1)\prod_{i=2}^Lp(a_i|a_{i-1})
$$
where $q(a)$ is the initial probability. It is \emph{stationary} if
$$
q(a)=\sum_{b\in\Sigma} q(b)p(a|b)
$$

Suppose there are $n$ states. Define square transition matrix
$$
\mathbf{P}=\left(\begin{array}{cccc}
p(1|1) & p(2|1) & \cdots & p(n|1) \\
p(1|2) & p(2|2) & \cdots & p(n|2) \\
\vdots & \vdots & \ddots & \vdots \\
p(1|n) & p(2|n) & \cdots & p(n|n)
\end{array}\right)
$$
and column vector
$$
\mathbf{q}=\left(\begin{array}{c}
q(1) \\
q(2) \\
\vdots\\
q(n)
\end{array}\right)
$$
Then in the matrix form:
$$
\mathbf{q}^\intercal\cdot\mathbf{P}=\mathbf{q}^\intercal
$$

\subsection{$k$-order Markov chain}
$$
\Pr\{X_i=a_i|X_{1,i-1}=a_1\cdots a_{i-1}\}=\Pr\{X_i=a_i|X_{i-k,i-1}=a_{i-k}\cdots a_{i-1}\}
$$

\newpage

\section{Hidden Markov Model (HMM)}
Let $\mathcal{S}$ be the state space. A homogeneous HMM is defined by:
$$
p(t|s)\triangleq\Pr\{Z_i=t|Z_{i-1}=s\}
$$
$$
e(a|s)\triangleq\Pr\{X_i=a|Z_i=s\}
$$

\subsection{The Viterbi algorithm}
$$
\gamma(i+1,t)=e(a_{i+1}|t)\max_s\gamma(i,s)p(t|s)
$$

\subsection{The forward algorithm}
\begin{eqnarray*}
\alpha(i,t)&\triangleq&\Pr\{Z_i=t,X_{1,i}=a_1\cdots a_i\}\\
&=&\sum_{s\in\mathcal{S}}\Pr\{Z_i=t,Z_{i-1}=s,X_{1,i}=a_1\cdots a_i\}\\
&=&\sum_s\Pr\{Z_i=t,X_i=a_i,Z_{i-1}=s,X_{1,{i-1}}=a_1\cdots a_{i-1}\}\\
&=&\sum_s\Pr\{Z_i=t,X_i=a_i|Z_{i-1}=s\}\Pr\{Z_{i-1}=s,X_{1,{i-1}}=a_1\cdots a_{i-1}\}\\
&=&\sum_s\Pr\{Z_i=t|Z_{i-1}=s\}\Pr\{X_i=a_i|Z_i=t\}\alpha(i-1,s)\\
&=&e(a_i|t)\sum_s\alpha(i-1,s)p(t|s)
\end{eqnarray*}
For convenience and consistency, we often assume at $i=0$, the HMM starts at
state $\epsilon$ at probability 1, i.e. $\Pr\{Z_0=\epsilon\}=1$. Then at $i=1$:
$$
\alpha(1,t)=\Pr\{Z_1=t,X_1=a_1\}=\Pr\{Z_1=t,X_1=a_1|Z_0=\epsilon\}=e(a_1|t)p(t|\epsilon)
$$
where $p(t|\epsilon)$ is arbitrary. In practice, we often choose
$p(t|\epsilon)=q(t)$.

The probability of a sequence
$$
P(x)=\Pr\{X_{1,L}=a_1\cdots a_L\}=\sum_s\Pr\{X_{1,L}=a_1\cdots a_L,Z_i=s\}=\sum_s\alpha(i,s)
$$

\subsection{The backward algorithm}
\begin{eqnarray*}
\beta(i,s)&\triangleq&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_i=s\}\\
&=&\sum_{t\in\mathcal{S}}\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L,Z_{i+1}=t|Z_i=s\}\\
&=&\sum_t\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_{i+1}=t\}\Pr\{Z_{i+1}=t|Z_i=s\}\\
&=&\sum_t\Pr\{X_{i+2,L}=a_{i+2}\cdots a_L|Z_{i+1}=t\}\Pr\{X_{i+1}=a_{i+1}|Z_{i+1}=t\}p(t|s)\\
&=&\sum_t\beta(i+1,t)e(a_{i+1}|t)p(t|s)
\end{eqnarray*}
Similarly, we assume all sequences end up with a special symbol
``{\tt\char36}'', i.e. $X_{L+1,L+1}=a_{L+1}={\tt\char36}$. Then
$$
\beta(L,s)=\Pr\{X_{L+1,L+1}={\tt\char36}|Z_L=s\}=1
$$

\subsection{Posterior decoding}
\begin{eqnarray*}
&& \Pr\{X_{1,L}=a_1\cdots a_L,Z_i=s\}\\
&=&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L,X_{1,i}=a_1\cdots a_i,Z_i=s\}\\
&=&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_i=s\}\Pr\{Z_i=s,X_{1,i}=a_1\cdots a_i\}\\
&=&\alpha(i,s)\beta(i,s)
\end{eqnarray*}
$$
\Pr\{Z_i=s|x\}=\frac{\alpha(i,s)\beta(i,s)}{P(x)}
$$
$$
\Pr\{Z_i=s,Z_{i+1}=t|x\}=\frac{1}{P(x)}\cdot\alpha(i,s)p(t|s)\beta(i+1,t)e(a_{i+1}|t)
$$

\subsection{Avoiding floating point underflow}

For large $L$, $\alpha(i,t)$ can become so small to be expressed with 64-bit
floating point numbers. Here shows one solution to this practical problem.
Define
$$
\tilde{\alpha}(i,t)\triangleq\frac{\alpha(i,t)}{\prod_{j=1}^is_j}
$$
Choose $\{s_i\}$ such that $\sum_t\tilde{\alpha}(i,t)=1$ for all $i$. The
forward equation becomes:
$$
\tilde{\alpha}(i,t)\prod_{j=1}^is_i=e(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)\prod_{j=1}^{i-1}s_j
$$
Then
$$
\tilde{\alpha}(i,t)=\frac{1}{s_i}\cdot e(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)
$$
Summing over $t$ on both sides, we have
$$
s_i=\sum_te(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)
$$
Similarly, define
$$
\tilde{\beta}(i,t)\triangleq\frac{\beta(i,t)}{\prod_{j=i}^Ls_j}
$$
The backward equation becomes
$$
\tilde{\beta}(i,s)\prod_{j=i}^Ls_j=\sum_t\tilde{\beta}(i+1,t)e(a_{i+1}|t)p(t|s)\prod_{j=i+1}^Ls_j
$$
i.e.:
$$
\tilde{\beta}(i,s)=\frac{1}{s_j}\cdot\sum_t\tilde{\beta}(i+1,t)e(a_{i+1}|t)p(t|s)
$$

\newpage

\section{Variant calling models}

\subsection{Genotype likelihood}
Let ${\bf d}$ be a list of read bases.
\begin{eqnarray*}
\Pr\{{\bf d}|G=\langle a,b\rangle\}
&=&\prod_i\Pr\{d_i|G=\langle a,b\rangle\}\\
&=&\prod_i\Big[\Pr\{d_i,\mbox{choose $a$ allele}|G=\langle a,b\rangle\}+\Pr\{d_i,\mbox{choose $b$ allele}|G=\langle a,b\rangle\}\Big]\\
&=&\prod_i\Big[\Pr\{d_i|\mbox{choose $a$ allele},G=\langle a,b\rangle\}\Pr\{\mbox{choose $a$ allele}|G=\langle a,b\rangle\}\\
&&+\Pr\{d_i,\mbox{choose $b$ allele}|G=\langle a,b\rangle\}\Big]\\
&=&\prod_i\frac{P(d_i|a)+P(d_i|b)}{2}
\end{eqnarray*}

An example. Suppose at a position there are 3 ``A'' bases and 7 ``C'' bases,
all at base quality 20, i.e. $\epsilon=0.01$.
\begin{eqnarray*}
\mathcal{L}(\langle{\tt A},{\tt A}\rangle)&=&\epsilon^{n_C}(1-\epsilon)^{n_A}\approx10^{-6}=Q60\\
\mathcal{L}(\langle{\tt C},{\tt C}\rangle)&=&\epsilon^{n_A}(1-\epsilon)^{n_C}\approx10^{-14}=Q140\\
\mathcal{L}(\langle{\tt A},{\tt C}\rangle)&=&\frac{1}{2^{n_A+n_C}}=2^{-10}\approx Q30
\end{eqnarray*}
Note that $-10\log_{10}0.5=3.010$, so $1/2$ is equivalent to Q3 approximately.

\subsection{Bayesian variant callers}
If ${\tt R}$ is the reference base, ${\tt A}$ is the alternate and $\theta$ is
the scaled mutation rate, under the Wright-Fisher model:
\begin{eqnarray*}
P(\langle{\tt R},{\tt A}\rangle)&=&\theta\\
P(\langle{\tt A},{\tt A}\rangle)&=&\frac{\theta}{2}\\
P(\langle{\tt R},{\tt R}\rangle)&=&1-\frac{3\theta}{2}\\
\end{eqnarray*}
For human, $\theta\approx10^{-3}$. The posterior:
$$
P(g|d)=\frac{P(d|g)P(g)}{\sum_hP(d|h)P(h)}=\frac{\mathcal{L}(g)P(g)}{\sum_h\mathcal{L}(h)P(h)}
$$

\newpage

\section{Multiple sequence alignment (MSA)}

\subsection{Scoring matrix for amino acids}

For a pair of aligned sequences with identity at least $L\%$, count each pair of
residues $a$ and $b$ in the alignment. Let $C_{ab}$ be the count across many
sequence pairs. Compute
$$
q_a=\frac{\sum_b C_{ab}}{\sum_{c,d}C_{cd}}
$$
$$
p_{ab}=\frac{C_{ab}}{\sum_{c,d}C_{cd}}
$$
The score $s(a,b)$ between residue $a$ and $b$ is computed as
$$
s(a,b)=\lambda\log\frac{p_{ab}}{q_aq_b}
$$
where $\lambda$ is a scaling factor. The resulting matrix is BLOSUM-$L$.

\subsection{Scoring a MSA}

Let $a_{ik}$ be the residue of the $i$-th sequence at column $k$. The
sum-of-pair column score is defined as
$$
S_k\triangleq\sum_{i<j}s(a_{ik},a_{jk})
$$
and the total alignment score is
$$
\mathcal{S}\triangleq\sum_k S_k
$$

Given a set of sequences, we can arbitrarily split them into two groups. The
column score in this case can be expressed as
$$
S_k=S^{(1)}_k+S^{(2)}_k+\sum_{a,b}n^{(1)}(a)n^{(2)}(b)s(a,b)
$$
where $S^{(1)}$ is the column score within the first group and $n^{(1)}(a)$ is
the number of residue $a$ in the first group.

\subsection{MSA}

Theoretical formulation: find the alignment such that $\mathcal{S}$ is the
largest. However, it has been proved that finding the optimal solution requires
$O(2^nL^n)$ time, where $n$ is the number of input sequences and $L$ is the
length of sequences. It is impractical to compute the optimal MSA.

In practice, the most popular algorithm is progressive MSA. We first build a
guide tree and then align a pair of groups at a time. This is extensively used
in classical MSA tools. Another popular class of algorithms is based on
partial-order alignment (POA). POA or simplified POA is more frequently used
for sequencing data.

\newpage

\section{Building phylogenetic trees}

There are four classes of tree building algorithms: distance-based,
maximum-parsimony, maximum-likelihood and Bayesian. We will only describe the
two most popular classes here: distance-based and maximum-likelihood.

\subsection{Distance-based algorithms}

Let $d_{ij}$ be the distance between sequence $i$ and $j$. The distance can be
derived from pairwise alignment, or more accurately, from MSA. For example:
\begin{verbatim}
    CAAT----CTAGTAC           CAATC----TAGTAC
    CAATCGAGATAGTAC           CAATCGAGATAGTAC
    CAAT--AGCTAGTAC           CAAT--AGCTAGTAC
        22222                     22223
\end{verbatim}
where the numbers below show the number of pairwise mismatches. The optimal
alignment is shown on the left. However, the optimal pairwise alignment between
the first two sequences is different.

Given $n$ sequences, there are $n(n+1)/2$ distances in a symmetric distance
matrix. However, given an unrooted tree, there are only $2n-3$ edges. The key
rationale behind distance-based methods is to fit more observations with fewer
parameters. Naively, we may use least-square to fit branch lengths given a tree
topology. In practice, this may lead to negative branch lengths.

Ths simplest practical algorithm is UPGMA, which is equivalent to hierarchical
clustering.  Let $C_i$ be a set of sequences. The distance between two sets of
sequences $C_i$ and $C_j$ is
$$
d_{ij}=\frac{1}{|C_i|\cdot|C_j|}\sum_{p\in C_i,q\in C_j}d_{pq}
$$
If we merge $C_i$ and $C_j$ into $C_k$, the distance between $C_k$ and another
set $C_l$ can be computed with
\begin{eqnarray*}
d_{kl}&=&\frac{1}{|C_k||C_l|}\sum_{p\in C_k,q\in C_l}d_{pq}\\
&=&\frac{1}{|C_k|}\cdot\left(\frac{1}{|C_l|}\sum_{p\in C_i,q\in C_l}d_{pq}+\frac{1}{|C_l|}\sum_{p\in C_j,q\in C_l}d_{pq}\right)\\
&=&\frac{1}{|C_i|+|C_j|}\left(d_{il}|C_i|+d_{jl}|C_j|\right)
\end{eqnarray*}

UPGMA assumes the presence of a molecular clock (the rate of evolution remains
the same across different clades). UPGMA trees ore often inconsistent with
phylogeny. The most widely used distance-based algorithm is neighbor-joining.

\subsection{Maximum-likelihood algorithms}

\subsubsection{Evolution between two sequences}

Given a set of discrete states (four nucleotides in case of DNA sequences),
The Kolmogorov forward equation discribes the probability of state changes over
time. Let $P_{ij}(s;t)$ be the probability of seeing state $i$ at time $s$ and
state $j$ at time $t$. The equation is
\[
\frac{\partial {\bf P}(s;t)}{\partial t}={\bf P}(s;t){\bf A}(t)
\]
where ${\bf P}(s;t)=(P_{ij}(s;t))_{n\times n}$ and
\[
{\bf A}(t)=\left.\frac{\partial {\bf P}(t,u)}{\partial u}\right|_{u=t}
\]
For a homogeneous process, ${\bf A}$ does not change with time $t$:
\[
{\bf P}'(t)={\bf P}(t){\bf A}
\]
\[
{\bf A}=\left.{\bf P}'(t)\right|_{t=0}
\]
On each row in ${\bf A}$, the sum of columns equals 0. ${\bf A}$ is called the
\emph{rate} matrix. \emph{Scaled mutation} rate has the same meaning.

Two-allele model:
\[
{\bf A}=\left(\begin{array}{cccc}
-\theta & \theta \\
\theta & -\theta \\
\end{array}\right)
\]
\[
{\bf P}(t)=\left(\begin{array}{cc}
r(t) & s(t) \\
s(t) & r(t) \\
\end{array}\right)
\]
\[
\left(\begin{array}{cc}
\dot{r}(t) & \dot{s}(t) \\
\dot{s}(t) & \dot{r}(t) \\
\end{array}\right)
=
\left(\begin{array}{cc}
r(t) & s(t) \\
s(t) & r(t) \\
\end{array}\right)
\cdot
\left(\begin{array}{cc}
-\theta & \theta \\
\theta & -\theta \\
\end{array}\right)
\]
We have
$$
\dot{r}=-r\theta+(1-r)\cdot\theta
$$
\[
r(t)=\frac{1}{2}\left(1+e^{-2\theta t}\right)
\]

Jukes-cantor model:
\[
{\bf A}=\left(\begin{array}{cccc}
-3\alpha & \alpha & \alpha & \alpha \\
\alpha & -3\alpha & \alpha & \alpha \\
\alpha & \alpha & -3\alpha & \alpha \\
\alpha & \alpha & \alpha & -3\alpha \\
\end{array}\right)
\]
\[
{\bf P}(t)=\left(\begin{array}{cccc}
r(t) & s(t) & s(t) & s(t) \\
s(t) & r(t) & s(t) & s(t) \\
s(t) & s(t) & r(t) & s(t) \\
s(t) & s(t) & s(t) & r(t) \\
\end{array}\right)
\]
\[
r(t)=\frac{1}{4}(1+3e^{-4\alpha t})
\]
\[
s(t)=\frac{1}{4}(1-e^{-4\alpha t})
\]

This is a \emph{reversible} model, which leads to an unrooted tree.
\subsubsection{Felsenstein's algorithm}

\begin{eqnarray*}
\alpha(v,a)&\triangleq&\Pr\{[v,a]\Rightarrow\sigma(T_v)\}\\
&=&\sum_{b,c}\Pr\big\{[v,a]\Rightarrow([u,b],[w,c])\Rightarrow(\sigma(T_u),\sigma(T_w))\big\}\\
&=&\sum_{b,c}\Pr\big\{[v,a]\Rightarrow([u,b],[w,c])\big\}\cdot\Pr\big\{[u,b]\Rightarrow\sigma(T_u),[w,c]\Rightarrow\sigma(T_w)\big\}\\
&=&\sum_{b,c}p_u(b|a,t_{vu})p_w(c|a,t_{vw})\alpha(u,b)\alpha(w,c)\\
&=&\left(\sum_b \alpha(u,b)p_u(b|a,t_{vu})\right)\cdot\left(\sum_c \alpha(w,c)p_w(c|a,t_{vw})\right)\\
&=&\alpha'(u,a)\alpha'(w,a)
\end{eqnarray*}
where
$$ \alpha'(u,a)=\sum_b p_u(b|a)\alpha(u,b) $$

\subsection{Practical concerns}

RAxML, PhyML and IQ-tree for ML. MrBayes for Bayesian. FastME for
distance-based.

Rooting. Long-branch attraction. NNI. Boostrapping and likelihood-ratio test.
Homolog, ortholog and paralog. dN/dS.  Duplication/loss inference. Incomplete
lineage sorting. Newick or New Hampshire format. Horizontal gene transfer.

\end{document}
