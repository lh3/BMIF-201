\documentclass[10pt]{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{comment}

\usepackage{hyperref}

\renewcommand{\ttdefault}{cmtt}

\begin{document}

\section{Probabilistic}

\subsection{Conditional probability and Bayesian formula}

Joint probability, written as $P(A\cap B)$ or $P(A,B)$, means two events happening at the
same time.  Conditional probability is defined as:
$$
P(A|B)\triangleq\frac{P(A,B)}{P(B)}
$$
Then
$$
P(A,B)=P(A|B)P(B)=P(B|A)P(A)
$$
thus Bayesian formula:
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

An example of joint and marginal probability. Let
$$
X=\left\{\begin{array}{ll}
1 & \mbox{if sunny} \\
0 & \mbox{otherwise}
\end{array}\right.
$$
$$
Y=\left\{\begin{array}{ll}
1 & \mbox{if warm} \\
0 & \mbox{if cold}
\end{array}\right.
$$
$$
\Pr\{\mbox{sunny}\}=\Pr\{\mbox{sunny and warm}\}+\Pr\{\mbox{sunny and cold}\}
$$
$$
\Pr\{X=x\}=\sum_y\Pr\{X=x,Y=y\}
$$
It is a common technique to add a latent variable if the original probability is hard to calculate.

\subsection{Likelihood and maximum likelihood estimation (MLE)}

A likelihood function or simply \emph{likelihood} is the probability of data
give parameters. It is a function of parameters.
$$
\mathcal{L}(\theta)=\Pr\{d|\theta\}
$$
MLE finds $\hat{\theta}$ that maximizes $\mathcal{L}(\theta)$.  The estimate
$\hat{\theta}$ satisfies:
$$
\left.\frac{\partial \mathcal{L}(\theta)}{\partial\theta}\right|_{\theta=\hat{\theta}}=0
$$
Or sometimes for the ease of derivaton:
$$
\left.\frac{\partial \log\mathcal{L}(\theta)}{\partial\theta}\right|_{\theta=\hat{\theta}}=0
$$

An example. Suppose after throwing a coin for $n$ times, we see heads for $m$ times.
What is the probability $p$ of seeing a head?
$$
\mathcal{L}(p)=P({\bf d}|p)=\prod_iP(d_i|p)=p^m\cdot(1-p)^{n-m}
$$
$$
\log\mathcal{L}(p)=m\log p+(n-m)\log(1-p)
$$
$$
\frac{\partial}{\partial p}\log\mathcal{L}=\frac{m}{p}-\frac{n-m}{1-p}
$$
The MLE $\hat{p}$ satisfies
$$
\left.\frac{\partial}{\partial p}\log\mathcal{L}(p)\right|_{p=\hat{p}}=0
$$
Solving the equation gives $\hat{p}=m/n$.

A second example. Suppose we throw a dice for many times and we see number $k$ for $n_k$ times.
What is the probabioity $p_k$ of seeing number $k$?
$$
\mathcal{L}({\bf p})=P({\bf d}|{\bf p})=\prod_iP(d_i|{\bf p})=\prod_{k=1}^6 p_k^{n_k}
$$
$$
\frac{\partial}{\partial p_k}\log\mathcal{L}({\bf p})=\frac{n_k}{p_k}
$$
We use the method of Lagrange multipliers to find the maximum on the constraint $\sum_kp_k=1$:
$$
\left.\frac{\partial}{\partial p_k}\left[\log\mathcal{L}({\bf p})-\lambda\left(\sum_lp_l-1\right)\right]\right|_{p_k=\hat{p}_k}=0
$$
Expanding this
$$
n_k=\lambda\cdot \hat{p}_k
$$
Suming over both sides
$$
\lambda=\sum_k n_k
$$
and thus
$$
\hat{p}_k = \frac{n_k}{\sum_ln_l}
$$

A third example.
Suppose we have two coins with head numbered ``0'' and tail numbered ``1''.
The chance of seeing 0 for the first coin is $p_{00}$ and the chance of seeing 0 for the second coin is $p_{10}$.
$p_{01}=1-p_{00}$ and $p_{11}=1-p_{10}$.
Now we toss both coins at the same time and sum the two numbers each time.
We get 0 for $n_0$ times, 1 for $n_1$ times and 2 for $n_2$ times.
What is the MLE of $p_{00}$ and $p_{11}$?
The likelihood function for this problem is
$$
\mathcal{L}({\bf p})=(p_{00}p_{10})^{n_0}\cdot(p_{01}p_{10}+p_{10}p_{01})^{n_1}\cdot(p_{01}p_{11})^{n_2}
$$
There is not an analytical solution to this maximization problem,
but in priciple, we can find numerical solutions.

\subsection{The estimation-maximization algorithm (EM)}

For a complex problem, it is often difficult to write down the close form of
$\mathcal{L}(\theta)$, so we can't directly do derivation. EM is an iterative
algorithm to find MLE when the likihood function can be expressed with
latent/hidden variables.
$$
\mathcal{L}(\theta)=\Pr\{{\bf d}|\theta\}=\sum_z\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}
$$
where $\sum_{\bf z}$ sums over all latent states. For contiguous latent
variables, replace sum with integral.

The central equation in EM is the $Q$ funciton defined by
$$
Q(\theta|\theta^t)\triangleq\mathbb{E}_{{\bf z}|{\bf d},\theta^t}\log\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}=\sum_{{\bf z}}\Pr\{{\bf Z}={\bf z}|{\bf d},\theta^t\}\cdot\log\Pr\{{\bf d},{\bf Z}={\bf z}|\theta\}
$$
As we will show in the next section, if we choose $\theta=\theta^{t+1}$ such that
$$
\left.\frac{\partial Q(\theta|\theta^t)}{\partial\theta}\right|_{\theta=\theta^{t+1}}=0
$$
we have $\mathcal{L}(\theta^{t+1})\ge\mathcal{L}(\theta^t)$.

For a problem solvable by EM, $Q(\theta|\theta^t)$ can usually be expressed in
the close form or take a simpler form than $\mathcal{L}(\theta)$. EM guarantees
convergence, but doesn't guarantee to converge to the global maxmium.

\subsection{Proving EM}

Here is the derivation of the EM algorithm in Durbin et al (1998). Because
$P(d,z|\theta)=P(z|d,\theta)P(d|\theta)$, we have
$$
\log P(d|\theta)=\log P(d,z|\theta)-\log P(z|d,\theta)
$$
Multiplying $P(z|d,\theta^t)$ and summing over $z$
$$
\log P(d|\theta)=\sum_zP(z|d,\theta^t)\log P(d,z|\theta) - \sum_zP(z|d,\theta^t)\log P(z|d,\theta)
$$
The first term on the right is the $Q$ function. Then
$$
\log P(d|\theta)-\log P(d|\theta^t)=Q(\theta|\theta^t)-Q(\theta^t|\theta^t)+\sum_z P(z|d,\theta^t)\log\frac{P(z|x,\theta^t)}{P(z|d,\theta)}
$$
The third term on the right is the relative entropy (aka Kullback-Leibler
distance), which is always non-negative. Therefore,
$$
\log P(d|\theta)-\log P(d|\theta^t)\ge Q(\theta|\theta^t)-Q(\theta^t|\theta^t)
$$
If we choose $\theta=\theta^{t+1}$ such that
$Q(\theta^{t+1}|\theta^t)>Q(\theta^t|\theta^t)$,
$P(d|\theta^{t+1})>P(d|\theta^t)$. i.e. $\theta^{t+1}$ guarantees increased
likelihood. For a standard EM, we typically choose $\theta^{t+1}$ at the $(t+1)$
iteration such that it maximizes $Q(\theta|\theta^t)$.

\section{Resolving ambiguously mapped reads for RNA-seq}

\subsection{Intuition}
Suppose we have $N$ transcripts and $R$ reads.
Define
$$
\delta_{it}=\left\{\begin{array}{ll}
1 & \mbox{if read $i$ hits transcript $t$} \\
0 & \mbox{otherwise}
\end{array}\right.
$$
Let $p_t$ be the fraction of transcript $t$, $t=1,\ldots,N$.
The probability of a read being sequenced from $t$ is
\begin{equation}\label{eq:p2q}
q_t=\frac{p_tl_t}{\sum_s{p_sl_s}}
\end{equation}
or conversely,
\begin{equation}\label{eq:q2p}
p_t=\frac{q_t/l_t}{\sum_s q_s/l_s}
\end{equation}
$q_t$ is also called the nucleotide fraction of transcript $t$.

Suppose we know $\{p_t\}$. The expected value $\{\bar{q}_t\}$ can be calculated with
$$
\bar{q}_t=\frac{1}{R}\sum_i\frac{p_t\delta_{it}}{\sum_s{p_s\delta_{is}}}
=\frac{1}{R}\sum_i\frac{(q_t/l_t)\cdot\delta_{it}}{\sum_s{(q_s/l_s)\cdot\delta_{is}}}
$$
With the EM procedure, we calculate
\begin{equation}\label{eq:rna-em}
q_t^{(k+1)}=\frac{1}{R}\sum_i\frac{\delta_{it}q_t^{(k)}/l_t}{\sum_s{\delta_{is}q_s^{(k)}/l_s}}
\end{equation}
until it converges.
We then derive the transcript fraction $\{p_t\}$ with Eq.~(\ref{eq:q2p}).
The TPM of transcript $t$ equals $p_t\cdot10^6$.

\subsection{Proving the RNA-seq EM}

The EM algorithm for RNA-seq quantification was first proposed by Li \emph{et
al.} (2010) and then popularized by kallisto (Bray \emph{et al.}, 2016) and
salmon (Patro \emph{et al.}, 2017).
However, the formulations between these papers are different.
The derivation here is again different from them.
Students are recommended to read these three papers.

For read $i$, let $Z_i$ be the transcript that read $i$ is originated.
It is a latent variable that cannot be observed.
We have $\Pr\{Z_i=t|\theta\}=q_t$ and further
\[
\log\Pr\{{\bf d},{\bf Z}={\bf t}|\theta\}=\log\Pr\{{\bf d}|{\bf Z}={\bf t}\}\Pr\{{\bf Z}={\bf t}|\theta\}=C+\sum_i\log q_{z_i}
\]
where $C$ is not a function of $\theta$.
To compute the EM Q function, we also need to compute the posterior
$$
\Pr\{Z_i=t|d_i,\theta\}=\frac{p_t\delta_{it}}{\sum_{s}p_s\delta_{is}}
$$
With these, we can derive
\begin{eqnarray*}
Q(\theta|\theta^k)&=&\sum_{{\bf t}}\Pr\{{\bf Z}={\bf t}|{\bf d},\theta^k\}\cdot\log\Pr\{{\bf d},{\bf Z}={\bf t}|\theta\}\\
&=&C+\sum_{{\bf t}}\prod_j\Pr\{Z_j=t_j|d_j,\theta^k\}\sum_i\log q_{t_i}\\
&=&C+\sum_{i=1}^R\sum_{t=1}^N\Pr\{Z_i=t|d_i,\theta^k\}\log q_t\\
\end{eqnarray*}
We use the method of Lagrange multipliers to find the optima of $Q$:
$$
\nabla_q\left[Q(\theta|\theta^k)-\lambda\left(\sum_sq_s-1\right)\right]=0
$$
Solving the equation gives
$$
q^{(k+1)}_t=\frac{1}{R}\sum_i\Pr\{Z_i=t|d_i,\theta^k\}=\frac{1}{R}\sum_i\frac{p^{(k)}_t\delta_{it}}{\sum_s{p^{(k)}_s\delta_{is}}}
=\frac{1}{R}\sum_i\frac{\delta_{it}q^{(k)}_t/l_t}{\sum_s{\delta_{is}q^{(k)}_s/l_s}}
$$
which is Eq.~(\ref{eq:rna-em}).

\newpage

\section{Markov Chain}

\subsection{Markov chain: the basic}
Let $\Sigma$ be an alphabet and $a\in\Sigma$ be a symbol. Markov property:
$$
\Pr\{X_i=a_i|X_{1,i-1}=a_1\cdots a_{i-1}\}=\Pr\{X_i=a_i|X_{i-1}=a_{i-1}\}
$$
A Markov chain is \emph{homogeneous} if
$$
\Pr\{X_i=a_i|X_{i-1}=a_{i-1}\}=\Pr\{X_j=a_j|X_{j-1}=a_{j-1}\}
$$
for any $i$ and $j$. Let $p_{ab}=p(b|a)\triangleq\Pr\{X_i=b|X_{i-1}=a\}$ be the
transition probability from $a$ to $b$. Then
$$
P(x)=\Pr\{X_{1,L}=a_1\cdots a_L\}=q(a_1)\prod_{i=2}^Lp(a_i|a_{i-1})
$$
where $q(a)$ is the initial probability. It is \emph{stationary} if
$$
q(a)=\sum_{b\in\Sigma} q(b)p(a|b)
$$

Suppose there are $n$ states. Define square transition matrix
$$
\mathbf{P}=\left(\begin{array}{cccc}
p(1|1) & p(2|1) & \cdots & p(n|1) \\
p(1|2) & p(2|2) & \cdots & p(n|2) \\
\vdots & \vdots & \ddots & \vdots \\
p(1|n) & p(2|n) & \cdots & p(n|n)
\end{array}\right)
$$
and column vector
$$
\mathbf{q}=\left(\begin{array}{c}
q(1) \\
q(2) \\
\vdots\\
q(n)
\end{array}\right)
$$
Then in the matrix form:
$$
\mathbf{q}^\intercal\cdot\mathbf{P}=\mathbf{q}^\intercal
$$

\subsection{$k$-order Markov chain}
$$
\Pr\{X_i=a_i|X_{1,i-1}=a_1\cdots a_{i-1}\}=\Pr\{X_i=a_i|X_{i-k,i-1}=a_{i-k}\cdots a_{i-1}\}
$$

\newpage

\section{Hidden Markov Model (HMM)}
Let $\mathcal{S}$ be the state space. A homogeneous HMM is defined by:
$$
p(t|s)\triangleq\Pr\{Z_i=t|Z_{i-1}=s\}
$$
$$
e(a|s)\triangleq\Pr\{X_i=a|Z_i=s\}
$$

\subsection{The Viterbi algorithm}
$\gamma(i,t)$ is the maximum probability of sequence $a_1\cdots a_i$ with $Z_i=t$.
$$
\gamma(i+1,t)=e(a_{i+1}|t)\max_s\gamma(i,s)p(t|s)
$$

\subsection{The forward algorithm}
\begin{eqnarray*}
\alpha(i,t)&\triangleq&\Pr\{Z_i=t,X_{1,i}=a_1\cdots a_i\}\\
&=&\sum_{s\in\mathcal{S}}\Pr\{Z_i=t,Z_{i-1}=s,X_{1,i}=a_1\cdots a_i\}\\
&=&\sum_s\Pr\{Z_i=t,X_i=a_i,Z_{i-1}=s,X_{1,{i-1}}=a_1\cdots a_{i-1}\}\\
&=&\sum_s\Pr\{Z_i=t,X_i=a_i|Z_{i-1}=s\}\Pr\{Z_{i-1}=s,X_{1,{i-1}}=a_1\cdots a_{i-1}\}\\
&=&\sum_s\Pr\{Z_i=t|Z_{i-1}=s\}\Pr\{X_i=a_i|Z_i=t\}\alpha(i-1,s)\\
&=&e(a_i|t)\sum_s\alpha(i-1,s)p(t|s)
\end{eqnarray*}
For convenience and consistency, we often assume at $i=0$, the HMM starts at
state $\epsilon$ at probability 1, i.e. $\Pr\{Z_0=\epsilon\}=1$. Then at $i=1$:
$$
\alpha(1,t)=\Pr\{Z_1=t,X_1=a_1\}=\Pr\{Z_1=t,X_1=a_1|Z_0=\epsilon\}=e(a_1|t)p(t|\epsilon)
$$
where $p(t|\epsilon)$ is arbitrary. In practice, we often choose
$p(t|\epsilon)=q(t)$.

The probability of a sequence
$$
P(x)=\Pr\{X_{1,L}=a_1\cdots a_L\}=\sum_s\Pr\{X_{1,L}=a_1\cdots a_L,Z_i=s\}=\sum_s\alpha(i,s)
$$

\subsection{The backward algorithm}
\begin{eqnarray*}
\beta(i,s)&\triangleq&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_i=s\}\\
&=&\sum_{t\in\mathcal{S}}\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L,Z_{i+1}=t|Z_i=s\}\\
&=&\sum_t\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_{i+1}=t\}\Pr\{Z_{i+1}=t|Z_i=s\}\\
&=&\sum_t\Pr\{X_{i+2,L}=a_{i+2}\cdots a_L|Z_{i+1}=t\}\Pr\{X_{i+1}=a_{i+1}|Z_{i+1}=t\}p(t|s)\\
&=&\sum_t\beta(i+1,t)e(a_{i+1}|t)p(t|s)
\end{eqnarray*}
Similarly, we assume all sequences end up with a special symbol
``{\tt\char36}'', i.e. $X_{L+1,L+1}=a_{L+1}={\tt\char36}$. Then
$$
\beta(L,s)=\Pr\{X_{L+1,L+1}={\tt\char36}|Z_L=s\}=1
$$

\subsection{Posterior decoding}
\begin{eqnarray*}
&& \Pr\{X_{1,L}=a_1\cdots a_L,Z_i=s\}\\
&=&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L,X_{1,i}=a_1\cdots a_i,Z_i=s\}\\
&=&\Pr\{X_{i+1,L}=a_{i+1}\cdots a_L|Z_i=s\}\Pr\{Z_i=s,X_{1,i}=a_1\cdots a_i\}\\
&=&\alpha(i,s)\beta(i,s)
\end{eqnarray*}
$$
\Pr\{Z_i=s|x\}=\frac{\alpha(i,s)\beta(i,s)}{P(x)}
$$
$$
\Pr\{Z_i=s,Z_{i+1}=t|x\}=\frac{1}{P(x)}\cdot\alpha(i,s)p(t|s)\beta(i+1,t)e(a_{i+1}|t)
$$

\subsection{Avoiding floating point underflow}

For large $L$, $\alpha(i,t)$ can become so small to be expressed with 64-bit
floating point numbers. Here shows one solution to this practical problem.
Define
$$
\tilde{\alpha}(i,t)\triangleq\frac{\alpha(i,t)}{\prod_{j=1}^is_j}
$$
Choose $\{s_i\}$ such that $\sum_t\tilde{\alpha}(i,t)=1$ for all $i$. The
forward equation becomes:
$$
\tilde{\alpha}(i,t)\prod_{j=1}^is_i=e(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)\prod_{j=1}^{i-1}s_j
$$
Then
$$
\tilde{\alpha}(i,t)=\frac{1}{s_i}\cdot e(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)
$$
Summing over $t$ on both sides, we have
$$
s_i=\sum_te(a_i|t)\sum_s\tilde{\alpha}(i-1,s)p(t|s)
$$
Similarly, define
$$
\tilde{\beta}(i,t)\triangleq\frac{\beta(i,t)}{\prod_{j=i}^Ls_j}
$$
The backward equation becomes
$$
\tilde{\beta}(i,s)\prod_{j=i}^Ls_j=\sum_t\tilde{\beta}(i+1,t)e(a_{i+1}|t)p(t|s)\prod_{j=i+1}^Ls_j
$$
i.e.:
$$
\tilde{\beta}(i,s)=\frac{1}{s_j}\cdot\sum_t\tilde{\beta}(i+1,t)e(a_{i+1}|t)p(t|s)
$$

\newpage

\section{Variant calling models}

\subsection{Genotype likelihood}
Let ${\bf d}$ be a list of read bases.
\begin{eqnarray*}
\Pr\{{\bf d}|G=\langle a,b\rangle\}
&=&\prod_i\Pr\{d_i|G=\langle a,b\rangle\}\\
&=&\prod_i\Big[\Pr\{d_i,\mbox{choose $a$ allele}|G=\langle a,b\rangle\}+\Pr\{d_i,\mbox{choose $b$ allele}|G=\langle a,b\rangle\}\Big]\\
&=&\prod_i\Big[\Pr\{d_i|\mbox{choose $a$ allele},G=\langle a,b\rangle\}\Pr\{\mbox{choose $a$ allele}|G=\langle a,b\rangle\}\\
&&+\Pr\{d_i,\mbox{choose $b$ allele}|G=\langle a,b\rangle\}\Big]\\
&=&\prod_i\frac{P(d_i|a)+P(d_i|b)}{2}
\end{eqnarray*}

An example. Suppose at a position there are 3 ``A'' bases and 7 ``C'' bases,
all at base quality 20, i.e. $\epsilon=0.01$.
\begin{eqnarray*}
\mathcal{L}(\langle{\tt A},{\tt A}\rangle)&=&\epsilon^{n_C}(1-\epsilon)^{n_A}\approx10^{-6}=Q60\\
\mathcal{L}(\langle{\tt C},{\tt C}\rangle)&=&\epsilon^{n_A}(1-\epsilon)^{n_C}\approx10^{-14}=Q140\\
\mathcal{L}(\langle{\tt A},{\tt C}\rangle)&=&\frac{1}{2^{n_A+n_C}}=2^{-10}\approx Q30
\end{eqnarray*}
Note that $-10\log_{10}0.5=3.010$, so $1/2$ is equivalent to Q3 approximately.

\subsection{Bayesian variant callers}
If ${\tt R}$ is the reference base, ${\tt A}$ is the alternate and $\theta$ is
the scaled mutation rate, under the Wright-Fisher model:
\begin{eqnarray*}
P(\langle{\tt R},{\tt A}\rangle)&=&\theta\\
P(\langle{\tt A},{\tt A}\rangle)&=&\frac{\theta}{2}\\
P(\langle{\tt R},{\tt R}\rangle)&=&1-\frac{3\theta}{2}\\
\end{eqnarray*}
For human, $\theta\approx10^{-3}$. The posterior:
$$
P(g|d)=\frac{P(d|g)P(g)}{\sum_hP(d|h)P(h)}=\frac{\mathcal{L}(g)P(g)}{\sum_h\mathcal{L}(h)P(h)}
$$

\newpage

\section{Multiple sequence alignment (MSA)}

\subsection{Scoring matrix for amino acids}

For a pair of aligned sequences with identity at least $L\%$, count each pair of
residues $a$ and $b$ in the alignment. Let $C_{ab}$ be the count across many
sequence pairs. Compute
$$
q_a=\frac{\sum_b C_{ab}}{\sum_{c,d}C_{cd}}
$$
$$
p_{ab}=\frac{C_{ab}}{\sum_{c,d}C_{cd}}
$$
The score $s(a,b)$ between residue $a$ and $b$ is computed as
$$
s(a,b)=\lambda\log\frac{p_{ab}}{q_aq_b}
$$
where $\lambda$ is a scaling factor. The resulting matrix is BLOSUM-$L$.

\subsection{Scoring a MSA}

Let $a_{ik}$ be the residue of the $i$-th sequence at column $k$. The
sum-of-pair column score is defined as
$$
S_k\triangleq\sum_{i<j}s(a_{ik},a_{jk})
$$
and the total alignment score is
$$
\mathcal{S}\triangleq\sum_k S_k
$$

Given a set of sequences, we can arbitrarily split them into two groups. The
column score in this case can be expressed as
$$
S_k=S^{(1)}_k+S^{(2)}_k+\sum_{a,b}n^{(1)}(a)n^{(2)}(b)s(a,b)
$$
where $S^{(1)}$ is the column score within the first group and $n^{(1)}(a)$ is
the number of residue $a$ in the first group.

\subsection{MSA}

Theoretical formulation: find the alignment such that $\mathcal{S}$ is the
largest. However, it has been proved that finding the optimal solution requires
$O(2^nL^n)$ time, where $n$ is the number of input sequences and $L$ is the
length of sequences. It is impractical to compute the optimal MSA.

In practice, the most popular algorithm is progressive MSA. We first build a
guide tree and then align a pair of groups at a time. This is extensively used
in classical MSA tools. Another popular class of algorithms is based on
partial-order alignment (POA). POA or simplified POA is more frequently used
for sequencing data.

\newpage

\section{Building phylogenetic trees}

There are four classes of tree building algorithms: distance-based,
maximum-parsimony, maximum-likelihood and Bayesian. We will only describe the
two most popular classes here: distance-based and maximum-likelihood.

\subsection{Distance-based algorithms}

Let $d_{ij}$ be the distance between sequence $i$ and $j$. The distance can be
derived from pairwise alignment, or more accurately, from MSA. For example:
\begin{verbatim}
    CAAT----CTAGTAC           CAATC----TAGTAC
    CAATCGAGATAGTAC           CAATCGAGATAGTAC
    CAAT--AGCTAGTAC           CAAT--AGCTAGTAC
        22222                     22223
\end{verbatim}
where the numbers below show the number of pairwise mismatches. The optimal
alignment is shown on the left. However, the optimal pairwise alignment between
the first two sequences is different.

Given $n$ sequences, there are $n(n+1)/2$ distances in a symmetric distance
matrix. However, given an unrooted tree, there are only $2n-3$ edges. The key
rationale behind distance-based methods is to fit more observations with fewer
parameters. Naively, we may use least-square to fit branch lengths given a tree
topology. In practice, this may lead to negative branch lengths.

Ths simplest practical algorithm is UPGMA, which is equivalent to hierarchical
clustering.  Let $C_i$ be a set of sequences. The distance between two sets of
sequences $C_i$ and $C_j$ is
$$
d_{ij}=\frac{1}{|C_i|\cdot|C_j|}\sum_{p\in C_i,q\in C_j}d_{pq}
$$
If we merge $C_i$ and $C_j$ into $C_k$, the distance between $C_k$ and another
set $C_l$ can be computed with
\begin{eqnarray*}
d_{kl}&=&\frac{1}{|C_k||C_l|}\sum_{p\in C_k,q\in C_l}d_{pq}\\
&=&\frac{1}{|C_k|}\cdot\left(\frac{1}{|C_l|}\sum_{p\in C_i,q\in C_l}d_{pq}+\frac{1}{|C_l|}\sum_{p\in C_j,q\in C_l}d_{pq}\right)\\
&=&\frac{1}{|C_i|+|C_j|}\left(d_{il}|C_i|+d_{jl}|C_j|\right)
\end{eqnarray*}

UPGMA assumes the presence of a molecular clock (the rate of evolution remains
the same across different clades). UPGMA trees ore often inconsistent with
phylogeny. The most widely used distance-based algorithm is neighbor-joining.

\subsection{Maximum-likelihood algorithms}

\subsubsection{Evolution between two sequences}

Given a set of discrete states (four nucleotides in case of DNA sequences),
The Kolmogorov forward equation discribes the probability of state changes over
time. Let $P_{ij}(s;t)$ be the probability of seeing state $i$ at time $s$ and
state $j$ at time $t$. The equation is
\[
\frac{\partial {\bf P}(s;t)}{\partial t}={\bf P}(s;t){\bf A}(t)
\]
where ${\bf P}(s;t)=(P_{ij}(s;t))_{n\times n}$ and
\[
{\bf A}(t)=\left.\frac{\partial {\bf P}(t,u)}{\partial u}\right|_{u=t}
\]
For a homogeneous process, ${\bf A}$ does not change with time $t$:
\[
{\bf P}'(t)={\bf P}(t){\bf A}
\]
\[
{\bf A}=\left.{\bf P}'(t)\right|_{t=0}
\]
On each row in ${\bf A}$, the sum of columns equals 0. ${\bf A}$ is called the
\emph{rate} matrix. \emph{Scaled mutation} rate has the same meaning.

Two-allele model:
\[
{\bf A}=\left(\begin{array}{cccc}
-\theta & \theta \\
\theta & -\theta \\
\end{array}\right)
\]
\[
{\bf P}(t)=\left(\begin{array}{cc}
r(t) & s(t) \\
s(t) & r(t) \\
\end{array}\right)
\]
\[
\left(\begin{array}{cc}
\dot{r}(t) & \dot{s}(t) \\
\dot{s}(t) & \dot{r}(t) \\
\end{array}\right)
=
\left(\begin{array}{cc}
r(t) & s(t) \\
s(t) & r(t) \\
\end{array}\right)
\cdot
\left(\begin{array}{cc}
-\theta & \theta \\
\theta & -\theta \\
\end{array}\right)
\]
We have
$$
\dot{r}=-r\theta+(1-r)\cdot\theta
$$
\[
r(t)=\frac{1}{2}\left(1+e^{-2\theta t}\right)
\]

Jukes-cantor model:
\[
{\bf A}=\left(\begin{array}{cccc}
-3\alpha & \alpha & \alpha & \alpha \\
\alpha & -3\alpha & \alpha & \alpha \\
\alpha & \alpha & -3\alpha & \alpha \\
\alpha & \alpha & \alpha & -3\alpha \\
\end{array}\right)
\]
\[
{\bf P}(t)=\left(\begin{array}{cccc}
r(t) & s(t) & s(t) & s(t) \\
s(t) & r(t) & s(t) & s(t) \\
s(t) & s(t) & r(t) & s(t) \\
s(t) & s(t) & s(t) & r(t) \\
\end{array}\right)
\]
\[
r(t)=\frac{1}{4}(1+3e^{-4\alpha t})
\]
\[
s(t)=\frac{1}{4}(1-e^{-4\alpha t})
\]

This is a \emph{reversible} model, which leads to an unrooted tree.
\subsubsection{Felsenstein's algorithm}

\begin{eqnarray*}
\alpha(v,a)&\triangleq&\Pr\{[v,a]\Rightarrow\sigma(T_v)\}\\
&=&\sum_{b,c}\Pr\big\{[v,a]\Rightarrow([u,b],[w,c])\Rightarrow(\sigma(T_u),\sigma(T_w))\big\}\\
&=&\sum_{b,c}\Pr\big\{[v,a]\Rightarrow([u,b],[w,c])\big\}\cdot\Pr\big\{[u,b]\Rightarrow\sigma(T_u),[w,c]\Rightarrow\sigma(T_w)\big\}\\
&=&\sum_{b,c}p_u(b|a,t_{vu})p_w(c|a,t_{vw})\alpha(u,b)\alpha(w,c)\\
&=&\left(\sum_b \alpha(u,b)p_u(b|a,t_{vu})\right)\cdot\left(\sum_c \alpha(w,c)p_w(c|a,t_{vw})\right)\\
&=&\alpha'(u,a)\alpha'(w,a)
\end{eqnarray*}
where
$$ \alpha'(u,a)=\sum_b p_u(b|a)\alpha(u,b) $$

\subsection{Practical concerns}

RAxML, PhyML and IQ-tree for ML. MrBayes for Bayesian. FastME for
distance-based.

Rooting. Long-branch attraction. NNI. Boostrapping and likelihood-ratio test.
Homolog, ortholog and paralog. dN/dS.  Duplication/loss inference. Incomplete
lineage sorting. Newick or New Hampshire format. Horizontal gene transfer.

\end{document}
